{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INdc6kxLsOlc"
      },
      "outputs": [],
      "source": [
        "!pip install datasets torch torchaudio librosa scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lTGMp4TzO23"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "import torch\n",
        "import IPython.display as ipd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcMISGXSQsf-"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zan0xfREvEBy"
      },
      "outputs": [],
      "source": [
        "!pip install gradio wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XynOYMsyt0II"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model, BertTokenizer, BertModel\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "import gradio as gr\n",
        "import torchaudio\n",
        "import wandb\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "hug_api_key=\"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\"\n",
        "login(hug_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSrOZP176C3-"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset = load_dataset(\"speech_commands\", \"v0.01\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cE4taMOAyeCo"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKD1fNuOtG7N"
      },
      "outputs": [],
      "source": [
        "from datasets import get_dataset_split_names\n",
        "\n",
        "train_dataset = dataset[\"train\"]\n",
        "val_dataset = dataset[\"validation\"]\n",
        "test_dataset = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atCIHtr4sYTu"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Use only half the training data\n",
        "half_length = len(train_dataset) // 2\n",
        "train_dataset = train_dataset.select(range(half_length))\n",
        "\n",
        "# Check size\n",
        "print(f\"Original training set size: {len(dataset['train'])}\")\n",
        "print(f\"Reduced training set size: {len(train_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nFBuFPudF1m"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1zfr43ldE5L"
      },
      "outputs": [],
      "source": [
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "example= dataset[\"train\"][0][\"audio\"]\n",
        "waveform=example[\"array\"]\n",
        "sample_rate=example[\"sampling_rate\"]\n",
        "\n",
        "#plotting waveform\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(waveform)\n",
        "plt.title(\"Waveform\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# to play the audio\n",
        "ipd.display(ipd.Audio(waveform, rate=sample_rate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P14VaQ11c_AA"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZvQXnU9hIDC"
      },
      "outputs": [],
      "source": [
        "#converting audio to melo spectrogram\n",
        "\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "\n",
        "#convert to mel spectrogram\n",
        "mel_spec=librosa.feature.melspectrogram(y=waveform,sr=sample_rate, n_mels= 128)\n",
        "\n",
        "#convert to db scale\n",
        "mel_spec_db=librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "#plot the spectrogram\n",
        "plt.figure(figsize=(10,4))\n",
        "librosa.display.specshow(mel_spec_db, sr=sample_rate, x_axis='time', y_axis='mel')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title(\"Mel Spectrogram\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Fy41oW8qGiZ"
      },
      "outputs": [],
      "source": [
        "#normalize the audio\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def normalize_waveform(waveform):\n",
        "   return waveform/np.max(np.abs(waveform))\n",
        "\n",
        "#Apply normalization\n",
        "\n",
        "waveform=normalize_waveform(waveform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIAwal6JqZCX"
      },
      "outputs": [],
      "source": [
        "# extract Mel-Frequency Cepstral Coefficient (features)\n",
        "mfccs=librosa.feature.mfcc(y=waveform, sr=sample_rate, n_mfcc=13)\n",
        "\n",
        "#plot mfcc\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "librosa.display.specshow(mfccs, x_axis='time')\n",
        "plt.colorbar()\n",
        "plt.title(\"MFCC\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxLc4A1800g3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import Wav2Vec2Model, BertModel, Wav2Vec2Processor, BertTokenizer\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import evaluate\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzLAQYCctQ98"
      },
      "outputs": [],
      "source": [
        "# Data preprocessing\n",
        "\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model, BertTokenizer, BertModel\n",
        "\n",
        "# Audio encoder\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "#audio_embedding_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "#audio_embedding_model.gradient_checkpointing_enable()\n",
        "# Text encoder\n",
        "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "#text_embedding_model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "#text_embedding_model.gradient_checkpointing_enable()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "916ZzTWIQfoG"
      },
      "outputs": [],
      "source": [
        "#dataset = load_dataset(\"speech_commands\", \"v0.01\")\n",
        "labels = train_dataset.features[\"label\"].names\n",
        "label2id = {l: i for i, l in enumerate(labels)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHniNMi1j6f-"
      },
      "outputs": [],
      "source": [
        "def preprocess(example):\n",
        "    # Extract the audio data\n",
        "    audio = example[\"audio\"]\n",
        "\n",
        "    # Get the label index (eg: 0, 1, 2)\n",
        "    label_idx = example[\"label\"]\n",
        "\n",
        "    # Convert the label index to the actual text label (eg: \"yes\", \"no\", etc.)\n",
        "    label_text = labels[label_idx]\n",
        "\n",
        "    # Process the raw audio array into model-ready input using a pre-trained audio processor\n",
        "    # Converts audio to tensor\n",
        "    # Pads/truncates to fixed length (16000 samples=1 sec at 16kHz)\n",
        "    # Generates attention mask to indicate real audio vs. padding\n",
        "    audio_input = processor(\n",
        "        audio[\"array\"],\n",
        "        sampling_rate=audio[\"sampling_rate\"],\n",
        "        return_tensors=\"pt\",           # Return as PyTorch tensors\n",
        "        padding=\"max_length\",          # Pad to fixed size\n",
        "        truncation=True,               # Truncate if too long\n",
        "        max_length=16000,              # Fixed length audio input\n",
        "        return_attention_mask=True     # Return attention mask\n",
        "    )\n",
        "\n",
        "    # Tokenize the text label into input IDs using a tokenizer (eg:BERT tokenizer)\n",
        "    text_input = tokenizer(\n",
        "        label_text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    # Return a dictionary of processed inputs(for the model)\n",
        "    return {\n",
        "        \"audio_input\": audio_input.input_values[0],         # Tensor of processed audio\n",
        "        \"audio_attention\": audio_input.attention_mask[0],   # Audio attention mask\n",
        "        \"text_input\": text_input.input_ids[0],              # Tensor of tokenized text\n",
        "        \"text_attention\": text_input.attention_mask[0],     # Text attention mask\n",
        "        \"label\": label_idx                                  # Original label index\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxUmK8nnPBhG"
      },
      "outputs": [],
      "source": [
        "# Model Architecture\n",
        "\n",
        "class MultimodalCommandClassifier(nn.Module):\n",
        "    def __init__(self, audio_model_name=\"facebook/wav2vec2-base\", text_model_name=\"bert-base-uncased\", hidden_dim=768, num_classes=35):\n",
        "        super().__init__()\n",
        "        self.audio_encoder = Wav2Vec2Model.from_pretrained(audio_model_name) #load pretrained audio encoder\n",
        "        self.text_encoder = BertModel.from_pretrained(text_model_name)# pretrained text encoder\n",
        "        self.audio_proj = nn.Linear(self.audio_encoder.config.hidden_size, hidden_dim) #Project audio features to a common hidden size\n",
        "        self.text_proj = nn.Linear(self.text_encoder.config.hidden_size, hidden_dim)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_dim, nhead=8)#to fuse audio and text representations\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=2)\n",
        "        self.classifier = nn.Linear(hidden_dim, num_classes)#Final classification layer to predict the command label\n",
        "\n",
        "    def forward(self, audio_input, audio_attention, text_input, text_attention):\n",
        "        audio_feat = self.audio_encoder(audio_input, attention_mask=audio_attention).last_hidden_state\n",
        "        audio_proj = self.audio_proj(audio_feat).permute(1, 0, 2)\n",
        "        text_feat = self.text_encoder(text_input, attention_mask=text_attention).last_hidden_state\n",
        "        text_proj = self.text_proj(text_feat).permute(1, 0, 2)\n",
        "        fused = self.transformer_decoder(tgt=audio_proj, memory=text_proj)\n",
        "        fused = fused.permute(1, 0, 2).mean(dim=1)\n",
        "        logits = self.classifier(fused)# to get logits for each class\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwZC_WRNYEsT"
      },
      "outputs": [],
      "source": [
        "# Apply map and filter nulls\n",
        "print(\"datasets \")\n",
        "train_dataset = train_dataset.map(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHpeUjBSdXrw"
      },
      "outputs": [],
      "source": [
        "# Handles batching of examples with different tensor types\n",
        "def collate_fn(batch):\n",
        "    collated = {}\n",
        "\n",
        "    # Iterate over keys like \"audio_input\", \"text_input\"\n",
        "    for key in batch[0]:\n",
        "        values = [example[key] for example in batch]\n",
        "\n",
        "        if isinstance(values[0], torch.Tensor):\n",
        "            # Stack tensor values into a batch (eg:[B, ...])\n",
        "            if key in [\"text_input\", \"label\"]:\n",
        "                collated[key] = torch.stack(values).long()  # Convert to LongTensor\n",
        "            else:\n",
        "                collated[key] = torch.stack(values)\n",
        "\n",
        "        elif isinstance(values[0], (int, float)):\n",
        "            # Convert list of scalars to tensor\n",
        "            collated[key] = torch.tensor(values)\n",
        "\n",
        "        elif isinstance(values[0], list):\n",
        "            # Handle lists (eg: if inputs are lists of ints)\n",
        "            if key in [\"text_input\", \"label\"] and all(isinstance(item, int) for item in values[0]):\n",
        "                collated[key] = torch.tensor(values, dtype=torch.long)\n",
        "            else:\n",
        "                collated[key] = torch.Tensor(values)\n",
        "\n",
        "    return collated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJWrfeSzeBNT"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u78u3wItuS7_"
      },
      "outputs": [],
      "source": [
        "# Initialize model and training config\n",
        "model = MultimodalCommandClassifier(num_classes=len(labels))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "metric_acc = evaluate.load(\"accuracy\")\n",
        "metric_f1 = evaluate.load(\"f1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2Ef1XpCKikD"
      },
      "outputs": [],
      "source": [
        "# Freeze both encoders\n",
        "for param in model.audio_encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model.text_encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Optimizer, loss, metrics\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "metric_acc = evaluate.load(\"accuracy\")\n",
        "metric_f1 = evaluate.load(\"f1\")\n",
        "\n",
        "# Trackers\n",
        "train_loss_values = []\n",
        "train_acc_values = []\n",
        "\n",
        "best_f1 = 0\n",
        "\n",
        "# Training loop\n",
        "start_time = time.time()\n",
        "for epoch in range(1, 4):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        for k in batch:\n",
        "            if isinstance(batch[k], torch.Tensor):\n",
        "                batch[k] = batch[k].to(device)\n",
        "\n",
        "        logits = model(batch[\"audio_input\"], batch[\"audio_attention\"],\n",
        "                       batch[\"text_input\"], batch[\"text_attention\"])\n",
        "        loss = loss_fn(logits, batch[\"label\"])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            metric_acc.add_batch(predictions=preds, references=batch[\"label\"])\n",
        "            metric_f1.add_batch(predictions=preds, references=batch[\"label\"])\n",
        "\n",
        "    # After all batches\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    train_metrics = metric_acc.compute()\n",
        "    train_f1 = metric_f1.compute(average=\"weighted\")\n",
        "\n",
        "    train_loss_values.append(avg_train_loss)\n",
        "    train_acc_values.append(train_metrics[\"accuracy\"])\n",
        "\n",
        "    print(f\"Epoch {epoch} | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_metrics['accuracy']:.4f} | Train F1: {train_f1['f1']:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Training Complete  | Total Training Time: {(end_time - start_time)/60:.2f} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUvEOvL1ubqm"
      },
      "outputs": [],
      "source": [
        "# Save model\n",
        "torch.save(model.state_dict(), \"best_multimodal_fusion_model.pt\")\n",
        "\n",
        "print(\"Training Complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLmVcOZfLMxa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VEY4Duk_N1n"
      },
      "outputs": [],
      "source": [
        "# Preprocess test set\n",
        "\n",
        "test_dataset = dataset[\"test\"].map(preprocess)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwpRHTVu_VY7"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "test_loss = 0\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        for k in batch:\n",
        "            if isinstance(batch[k], torch.Tensor):\n",
        "                batch[k] = batch[k].to(device)\n",
        "\n",
        "        logits = model(batch[\"audio_input\"], batch[\"audio_attention\"],\n",
        "                       batch[\"text_input\"], batch[\"text_attention\"])\n",
        "        loss = loss_fn(logits, batch[\"label\"])\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        y_true.extend(batch[\"label\"].cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "# Compute Accuracy & F1\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "test_accuracy = accuracy_score(y_true, y_pred)\n",
        "test_f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "\n",
        "print(f\"\\nTest Loss: {avg_test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | Test F1 Score: {test_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nL9_AHYMuiuW"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(labels))))\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "disp.plot(xticks_rotation=45, ax=ax)\n",
        "plt.title(\"Test Set Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"test_confusion_matrix.png\")\n",
        "plt.close()\n",
        "\n",
        "# Plot test metrics\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar([\"Loss\", \"Accuracy\", \"F1\"], [avg_test_loss, test_accuracy, test_f1], color=[\"tomato\", \"steelblue\", \"seagreen\"])\n",
        "plt.title(\"Test Set Metrics\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.ylim(0, 1)\n",
        "for i, v in enumerate([avg_test_loss, test_accuracy, test_f1]):\n",
        "    plt.text(i, v + 0.02, f\"{v:.2f}\", ha='center')\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"test_metrics_plot.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiU-d1Dd_ncU"
      },
      "outputs": [],
      "source": [
        "def predict_command(audio_path):\n",
        "    waveform, sr = torchaudio.load(audio_path)\n",
        "\n",
        "    if sr != 16000:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    audio_input = processor(\n",
        "        waveform.squeeze(), sampling_rate=16000, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=16000\n",
        "    )\n",
        "\n",
        "    input_audio = audio_input.input_values.to(device)\n",
        "    attention_audio = audio_input.attention_mask.to(device)\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    for label in labels:\n",
        "        tokenized = tokenizer(label, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
        "        input_text = tokenized.input_ids.to(device)\n",
        "        attention_text = tokenized.attention_mask.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_audio, attention_audio, input_text, attention_text)\n",
        "            all_logits.append(logits.squeeze(0))\n",
        "\n",
        "    logits_stack = torch.stack(all_logits)\n",
        "    probs = torch.nn.functional.softmax(logits_stack.mean(dim=0), dim=0)\n",
        "    pred_idx = torch.argmax(probs).item()\n",
        "    pred_label = id2label[pred_idx]\n",
        "    confidence = probs[pred_idx].item()\n",
        "\n",
        "    return pred_label, round(confidence * 100, 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdsEERBQKkT4"
      },
      "outputs": [],
      "source": [
        "import torchaudio\n",
        "\n",
        "def predict_command(audio_file_path):\n",
        "    # Load audio from file\n",
        "    waveform, sample_rate = torchaudio.load(audio_file_path)\n",
        "\n",
        "    # Resample if necessary\n",
        "    if sample_rate != 16000:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    # Make sure shape [1, 16000]\n",
        "    if waveform.shape[1] < 16000:\n",
        "        waveform = torch.nn.functional.pad(waveform, (0, 16000 - waveform.shape[1]))\n",
        "    else:\n",
        "        waveform = waveform[:, :16000]\n",
        "\n",
        "    # Preprocess with processor\n",
        "    audio_input = processor(\n",
        "        waveform.squeeze(0),\n",
        "        sampling_rate=16000,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"longest\"\n",
        "    )\n",
        "\n",
        "    input_audio = audio_input.input_values.to(device)\n",
        "\n",
        "    # Check if attention_mask exists\n",
        "    if \"attention_mask\" in audio_input:\n",
        "        attention_audio = audio_input.attention_mask.to(device)\n",
        "    else:\n",
        "        attention_audio = torch.ones_like(input_audio, dtype=torch.long).to(device)\n",
        "\n",
        "    # Create dummy text input (until you connect Pinecone retrieval)\n",
        "    dummy_text = tokenizer(\"dummy\", return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
        "    input_text = dummy_text.input_ids.to(device)\n",
        "    attention_text = dummy_text.attention_mask.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_audio, attention_audio, input_text, attention_text)\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        top_pred = torch.argmax(probs, dim=-1)\n",
        "        confidence = probs[0, top_pred].item()\n",
        "\n",
        "    pred_label = labels[top_pred.item()]\n",
        "    return pred_label, confidence * 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCGcdTZHSTv-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ----- Gradio UI -----\n",
        "def gradio_interface(audio):\n",
        "    pred_label, confidence = predict_command(audio)\n",
        "    return f\" Predicted Command: {pred_label}\\n Confidence: {confidence:.2f}%\"\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=gr.Audio(type=\"filepath\", label=\"Upload or Record Audio\"),\n",
        "    outputs=gr.Textbox(label=\"Predicted Command & Confidence\"),\n",
        "    title=\"Spoken Command Classifier\",\n",
        "    description=\"Upload or record a spoken command\"\n",
        ")\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRIQv_YVNvDU"
      },
      "outputs": [],
      "source": [
        "!pip install nbformat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1EHzSreNzfa"
      },
      "outputs": [],
      "source": [
        "import nbformat\n",
        "\n",
        "filename = \"/content/drive/MyDrive/Colab Notebooks/SpokenCommandClassification_Project2.ipynb\"  # Replace with your notebook name\n",
        "\n",
        "with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "    nb = nbformat.read(f, as_version=4)\n",
        "\n",
        "if \"widgets\" in nb[\"metadata\"]:\n",
        "    if \"state\" not in nb[\"metadata\"][\"widgets\"]:\n",
        "        nb[\"metadata\"][\"widgets\"][\"state\"] = {}\n",
        "\n",
        "# Save the corrected notebook\n",
        "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    nbformat.write(nb, f)\n",
        "\n",
        "print(\"Notebook fixed âœ…\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}