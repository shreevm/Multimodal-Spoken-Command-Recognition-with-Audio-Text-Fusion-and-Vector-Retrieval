{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnHHe_sofsfK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model, BertTokenizer, BertModel\n",
        "import torchaudio\n",
        "import pinecone\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from multimodal_training_pipeline import MultimodalCommandClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pinecone setup\n",
        "pinecone.init(api_key=\"\", environment=\"us-east-1\")\n",
        "index = pinecone.Index(\"voice-command-index\")\n"
      ],
      "metadata": {
        "id": "CdNcK1Icf2CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load processors and model\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "labels = [\"turn on light\", \"turn off light\", \"play music\", \"stop music\", \"increase volume\", \"decrease volume\", \"open window\", \"close window\", \"set alarm\", \"what time is it\"]"
      ],
      "metadata": {
        "id": "g152358-g4zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultimodalCommandClassifier(num_classes=len(labels))\n",
        "model.load_state_dict(torch.load(\"multimodal_spoken_cmd_model.pt\", map_location=torch.device(\"cpu\")))\n",
        "model.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Load evaluation metrics\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "f1 = evaluate.load(\"f1\")\n"
      ],
      "metadata": {
        "id": "e0aod6iPf4gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_command(audio_path):\n",
        "    waveform, sr = torchaudio.load(audio_path)\n",
        "    audio_inputs = processor(waveform.squeeze(), sampling_rate=sr, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
        "    audio_input = audio_inputs.input_values.to(device)\n",
        "    audio_attention = audio_inputs.attention_mask.to(device)\n",
        "\n",
        "    # For each label, run BERT encoder\n",
        "    all_logits = []\n",
        "    for label_text in labels:\n",
        "        text_inputs = tokenizer(label_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
        "        text_input = text_inputs.input_ids.to(device)\n",
        "        text_attention = text_inputs.attention_mask.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(audio_input, audio_attention, text_input, text_attention)\n",
        "            all_logits.append(logits.squeeze(0))\n",
        "\n",
        "    all_logits = torch.stack(all_logits)\n",
        "    pred_index = torch.argmax(all_logits.mean(dim=0)).item()\n",
        "    return labels[pred_index]\n"
      ],
      "metadata": {
        "id": "tBmuBunUg7aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Benchmark over test set\n",
        "def evaluate_on_testset(test_dataset):\n",
        "    preds, ground_truth = [], []\n",
        "    for item in test_dataset:\n",
        "        pred = predict_command(item[\"audio\"][\"path\"])\n",
        "        label = item[\"label\"]\n",
        "        preds.append(pred)\n",
        "        ground_truth.append(label)"
      ],
      "metadata": {
        "id": "chlLK4ohhCjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    acc = accuracy.compute(predictions=preds, references=ground_truth)\n",
        "    f1_score = f1.compute(predictions=preds, references=ground_truth, average=\"weighted\")\n",
        "    print(f\"Saved Model Inference Accuracy: {acc['accuracy']:.4f}, F1 Score: {f1_score['f1']:.4f}\")\n",
        "    return acc, f1_score\n"
      ],
      "metadata": {
        "id": "CR6kIGSThGkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio"
      ],
      "metadata": {
        "id": "_Zg_xOMLiITb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict command using Pinecone\n",
        "\n",
        "def encode_audio(audio_path):\n",
        "    waveform, sr = torchaudio.load(audio_path)\n",
        "    inputs = wav_proc(waveform.squeeze(), sampling_rate=sr, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        emb = wav_model(**inputs).last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "    return emb\n",
        "\n",
        "\n",
        "def top_k_predictions(audio_path, k=3):\n",
        "    audio_vec = encode_audio(audio_path)\n",
        "    res = index.query(vector=audio_vec, top_k=k, include_metadata=True)\n",
        "    predictions = [(match['id'], match['score']) for match in res['matches']]\n",
        "    return predictions\n",
        "\n",
        "# UI function\n",
        "def gradio_pipeline(audio):\n",
        "    waveform, sr = torchaudio.load(audio)\n",
        "    preds = top_k_predictions(audio, k=3)\n",
        "    pred_text = \"\\n\".join([f\"{cmd}: {score:.3f}\" for cmd, score in preds])\n",
        "    return waveform.squeeze().numpy(), pred_text\n",
        "\n",
        "# Gradio app\n",
        "audio_input = gr.Audio(type=\"filepath\", label=\"Input Audio\")\n",
        "wave_output = gr.Plot(label=\"Waveform\")\n",
        "text_output = gr.Textbox(label=\"Top-3 Predicted Commands with Scores\")\n",
        "\n",
        "app = gr.Interface(\n",
        "    fn=gradio_pipeline,\n",
        "    inputs=audio_input,\n",
        "    outputs=[wave_output, text_output],\n",
        "    title=\"Multimodal Spoken Command Inference with Pinecone\",\n",
        "    description=\"Upload or record audio. Predict top-k commands using Wav2Vec2 + Pinecone retrieval.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.launch()\n"
      ],
      "metadata": {
        "id": "Eq_UuTLBhNQl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}